
These notes summarize the lecture notes from the Linear Modelling course at Sheffield's School of Mathematics and Statistics, MSc degree programme. The original notes were written by Dr.\ Jeremy Oakley. This summary is completely derived from these notes and from other MSc sources. Any errors are most probably mine.

Everything is in matrix form unless a lower case letter with a subscript (such as $x_i$) is used (even there, I might deviate from this convention if I need to index sub-matrices; it's best to look at the context to decide what is meant).

\section{Some basic types of linear mixed model and their variance components}

\subsection{Varying intercepts model}

<<>>=
library(lme4)
fm1<-lmer(wear~material+(1|Subject),BHHshoes)
ranef(fm1)
@

The model is:

\begin{equation}
Y_{ijk} = \beta_j + b_{i}+\epsilon_{ijk}
\end{equation}

\noindent
$i=1,\dots,10$ is subject id, $j=1,2$ is the factor level, $k$ is the number of replicates (here 1).
$b_i \sim N(0,\sigma_b^2), \epsilon_{ijk}\sim N(0,\sigma^2)$.

The general form for any model in this case is:

\begin{equation}
\begin{pmatrix}
Y_{i1}\\
Y_{i2}
\end{pmatrix}
\sim
N\left(
\begin{pmatrix}
\beta_1\\
\beta_2\\
\end{pmatrix}
,
V
\right)
\end{equation}

where $V =\begin{pmatrix}
\sigma_b^2 + \sigma^2 & \sigma_b^2\\
\sigma_b^2 & \sigma_b^2 + \sigma^2\\
\end{pmatrix}
=
\begin{pmatrix}
\sigma^2_{b} + \sigma^2  &  \rho\sigma_{b}\sigma_{b}\\
\rho\sigma_{b}\sigma_{b} & \sigma^2_{b}+\sigma^2  \\       
\end{pmatrix}$.

We can recover these variance components as follows:

<<>>=
VarCorr(fm1)
@

$\hat{V}$ is therefore:

\begin{equation}
\begin{pmatrix}
\hat{\sigma}^2_{b} + \hat{\sigma}^2  &  \hat{\rho}\hat{\sigma}_{b}\hat{\sigma}_{b}\\
\hat{\rho}\hat{\sigma}_{b}\hat{\sigma}_{b} & \hat{\sigma}^2_{b}+\hat{\sigma}^2  \\       
\end{pmatrix}=
\begin{pmatrix}
2.47^2 + 0.27376^2 & 2.47^2\\
2.47^2 & 2.47^2 + 0.27376^2\\
\end{pmatrix}
\end{equation}

Note: $\hat{\rho}=1$ because the off-diagonal is $2.47^2=1\times 2.47 \times 2.47$. But this correlation is not estimated in the varying intercepts model.

\subsection{Varying intercepts and slopes (with correlation)}

<<>>=
fm2<-lmer(wear~material+(1+material|Subject),BHHshoes)
ranef(fm2)

VarCorr(fm2)
@

The model is 
\begin{equation}
Y_{ijk} = \beta_j + b_{ij}+\epsilon_{ijk}
\end{equation}

\noindent
$b_{ij}\sim N(0,\sigma_b)$. The variance $\sigma_b$ must be a $2\times 2$ matrix:

\begin{equation}
\begin{pmatrix}
\sigma_1^2 & \rho \sigma_1 \sigma_2\\
\rho \sigma_1 \sigma_2 & \sigma_2^2\\
\end{pmatrix}
\end{equation}

We can recover this from the random effects:

<<>>=
var(ranef(fm2)$Subject)
@

Note that $1\times \sqrt{5.90124}\times \sqrt{0.0045344}=0.16358$, which is how we get that $\hat{\rho}=1$.

The general form for the model is:

\begin{equation}
\begin{pmatrix}
Y_{i1}\\
Y_{i2}
\end{pmatrix}
\sim
N\left( 
\begin{pmatrix}
\beta_1\\
\beta_2\\
\end{pmatrix}
,
V
\right)
\end{equation}

where 

\begin{equation}
V =
%\begin{pmatrix}
%\sigma_b^2 + \sigma^2 & \sigma_b^2\\
%\sigma_b^2 & \sigma_b^2 + \sigma^2
%\end{pmatrix}=
\begin{pmatrix}
\sigma^2_{b,A} + \sigma^2  &  \rho\sigma_{b,A}\sigma_{b,B}\\
\rho\sigma_{b,A}\sigma_{b,B} & \sigma^2_{b,B}+\sigma^2  \\       
\end{pmatrix}
\end{equation}

And that's equal to (see VarCorr output above):


\begin{equation}
\begin{pmatrix}
5.93634+0.07266  & \rho\sigma_{b,A}\sigma_{b,B}=0.1645525\\
 0.1645525 & 0.0045617+ 0.07266\\
\end{pmatrix}
\end{equation}


Note that $\hat{\rho}$ is shown in VarCorr output (at the bottom) and can be computed since $\frac{Covar}{\sigma_a\times \sigma_b}=\rho$ and we know all the quantities on the LHS:

<<>>=
0.1645525/(sqrt(5.93634)*sqrt(0.0045617))
@

How to recover, from V, the correlation of 1 in the lmer random effects output of fm2? Is that 1 supposed to represent 0.99996?

\subsection{No varying intercepts, only slopes for each level}

<<>>=
fm3<-lmer(wear~material-1 + (material-1|Subject),BHHshoes)
ranef(fm3)
@

The model is

\begin{equation}
Y_{ijk} = \beta_j + b_{ij} + \epsilon_{ijk} 
\end{equation}

The random effects are:

$b_{ij}=\begin{pmatrix}
b_{i1}\\
b_{i12}
\end{pmatrix}
\sim N(0,\sigma_b^2)$, where $\sigma_b^2=
\begin{pmatrix}
\sigma_1^2 & \rho\sigma_1 \sigma_2 \\
\rho\sigma_1 \sigma_2 & \sigma_2^2 \\
\end{pmatrix}$. 

We can recover these values from:

<<>>=
var(ranef(fm3)$Subject)
@

$\hat{\rho}$ is 1 because $1\times 
\sqrt{5.9012}*\sqrt{6.2329}=6.0648$.


Here, V is

\begin{equation}
V =
%\begin{pmatrix}
%\sigma_b^2 + \sigma^2 & \sigma_b^2\\
%\sigma_b^2 & \sigma_b^2 + \sigma^2
%\end{pmatrix}=
\begin{pmatrix}
\sigma^2_{b,A} + \sigma^2  &  \rho\sigma_{b,A}\sigma_{b,B}\\
\rho\sigma_{b,A}\sigma_{b,B} & \sigma^2_{b,B}+\sigma^2  \\       
\end{pmatrix}
\end{equation}

\textbf{Note that the interpretation of the random effects is different from fm2: here, a random effect is computed for each material separately.}


From the VarCorr output, we have $\hat{V}$:

\begin{equation}
\begin{pmatrix}
5.9363 + 0.26956^2  &  1 \times 2.4365\times 2.5040 \\
1 \times 2.4365\times 2.5040 & 6.27+0.26956^2  \\       
\end{pmatrix}
\end{equation}


One insight is that V can be derived from the random effects variance components, and the error term's variance component:

\begin{equation}
V=
\begin{pmatrix}
\sigma^2_{b,A} &\rho\sigma_{b,A}\sigma_{b,B}\\
\rho\sigma_{b,A}\sigma_{b,B} & \sigma^2_{b,B}\\
\end{pmatrix}
+
\begin{pmatrix}
\sigma^2 & 0\\
0 & \sigma^2\\
\end{pmatrix}
\end{equation}

\subsection{Nested models (e.g., Worker/Machine)}

The model is:

\begin{equation}
Y_{ijk} = \beta_j + b_i + b_{ij} + \epsilon_{ijk}
\end{equation}

Here, we force force all random effects to be independent.
Observations between workers are independent, but observations on the same worker are correlated.

$b_i \sim N(0,\sigma_1^2), b_{ij} \sim N(0,\sigma_2^2)$, and $\epsilon\sim N(0,\sigma^2)$. $i$ is Worker, $j$ is machine, and $k$ is replicate.  

<<>>=
fm1<-lmer(score~Machine-1+(1|Worker/Machine),
data=Machines)
@

The variance components in fm1:

\small
\begin{tabular}{rlll}
  \hline
Comp.\ & Groups & Name & Var\\ 
  \hline
$\hat{\sigma}_2^2$ & Machine:Worker & (Int) & 13.909  \\ 
$\hat{\sigma}_1^2$  & Worker & (Int) & 22.858  \\ 
$\hat{\sigma}^2$& Res &  &  0.925 \\ 
   \hline
\end{tabular}

Number of obs: 54, groups: Machine:Worker, 18; Worker, 6.

\normalsize

For observations on Worker $i$, 

\begin{equation}
Var(Y_{ijk})= \sigma_1^2 + \sigma_2^2 + \sigma^2 
\end{equation}

Variance between machines within workers:
\begin{equation}
Covar(Y_{ijk},Y_{ijk'})= \sigma_1^2 + \sigma_2^2
\end{equation}

Variance between workers:
\begin{equation}
Covar(Y_{ijk},Y_{ij'k'})= \sigma_1^2
\end{equation}

Note:

1. $\hat{\sigma}_1^2$ all observations have the same variance;

2. $\hat{\sigma}_2^2$: the covariance between observations corresponding to the same worker using different machines is the same, for any pair of machines.


\begin{verbatim}
> ranef(fm1)
$`Machine:Worker`    $Worker
    (Intercept)
A:6     1.91609     6 -7.514666
A:2     1.55253     2 -1.375925
\end{verbatim}

In this model, the sum of the random effects for Worker 1 on Machine A is

$s_1 = b_1 + b_{11}$

\begin{verbatim}
> ranef(fm1)
...
     $`Machine:Worker`      $Worker
        (Intercept)
s1 = A:1    -0.75012 +      1.044598 = 0.29448
\end{verbatim}

and for Worker 1 on machine B,

$s_2 = b_1 + b_{21}$.

\begin{verbatim}
> ranef(fm1)
...
    $`Machine:Worker`      $Worker
     (Intercept)
s2 = B:1     1.50002 +      1.044598 =  2.5446
\end{verbatim}

For all Workers and machines, we can obtain these random effects $s$ from this matrix:

<<>>=
mat<-matrix(
  unlist(ranef(fm1)$`Machine:Worker`),6,3) 
+ 
  matrix(unlist(ranef(fm1)$Worker),6,3)
@
<<echo=F>>=
rownames(mat)<-c(6,2,4,1,3,5)
colnames(mat)<-LETTERS[1:3]
@

<<>>=
mat
@

Using lmer, we have $b_{i}$ and $b_{ij}$ independent, but $s_1$ and $s_2$ are
correlated via the common term $b_1$. We can recover the correlations between machine through the vcov matrix of the random effects (BLUPs) (\textbf{but note that we never see this in the lmer output---what's the significance of the fact that these are correlated?}):

<<>>=
var(mat)
@





\subsection{Varying intercepts and slopes (no correlation)}

\begin{equation}
Y_{ijk} = \beta_j + b_{ij} + \epsilon_{ijk}
\end{equation}



<<>>=
fm3<-lmer(score~Machine-1+
            (Machine-1|Worker),
          data=Machines)
@

<<>>=
ranef(fm3)
@

The random effects for Worker 1 on Machine A is

$s_1 = b_{11}=0.31199$

and for Worker 1 on Machine B,

$s_2 = b_{12}=2.55323$.

The `Machine independent' Worker random effect (varying intercept) $b_i$ has been dropped. 
We have $b_{11}$ correlated with $b_{12}$. We can see this when we recover the (co-)variances between machines from the random effects: 

\small
<<>>=
var(ranef(fm3)$Worker)
@
\normalsize

Also, the variances for each machine (16, 74, 18)
are also allowed to be different. Here are the variance components:

\tiny
\begin{tabular}{rlllll}
  \hline
Comp.\ & Groups & Name & Variance  & Corr$_{1,\cdot}$ &  Corr$_{2,\cdot}$  \\ 
  \hline
$\hat{\sigma}_{j=1}^2$ & Worker & A & 16.640   &    \\ 
$\hat{\sigma}_{j=2}^2$ &  & B & 74.395 & 
$\hat{\rho}_{1,2}$ 0.803 &    \\ 
$\hat{\sigma}_{j=3}^2$ &  & C & 19.268 &
$\hat{\rho}_{1,3}$ 0.623 & $\hat{\rho}_{2,3}$ 0.771   \\ 
$\hat{\sigma}^2$ & Res &  &  0.925 &  &    \\ 
   \hline
\end{tabular}
\normalsize


\begin{equation}
Var(Y_{ijk})= \sigma_j^2 + \sigma^2
\end{equation}

\begin{equation}
Covar(Y_{ijk},Y_{ijk'})= \sigma_j^2
\end{equation}

\begin{equation}
Covar(Y_{ijk},Y_{ij'k'})= \rho_{j,j'} \sigma_j\sigma_{j'}
\end{equation}



Note that the BLUPs' vcov matrix reflects the estimated values:

<<>>=
diag(var(ranef(fm3)$Worker))
cor(ranef(fm3)$Worker)
# look at the fm3 output 
## (the random effects table)
@


1. $\hat{\sigma}_j^2$ the variance of an observation depends on the machine being used; 

2. $\rho_{j,j'} \sigma_j\sigma_{j'}$ the covariance between observations corresponding to the same worker using different
machines is different, for different pairs of machines.

\begin{verbatim}
> var(ranef(fm3)$Worker)
         MachineA MachineB MachineC
MachineA   16.347   28.239   11.146
MachineB   28.239   74.093   29.181
MachineC   11.146   29.181   18.972
\end{verbatim}

\begin{equation}
\begin{pmatrix}
\sigma_{A}^2 & Cov_{A,B}     & Cov_{A,C}\\  
               & \sigma_{B}^2 & Cov_{B,C} \\
              &               & \sigma_{C}^2\\
\end{pmatrix}
\end{equation}

Note that, for given machines $j$ and $j'$, say A, B: 

$Covar(Y_{ijk},Y_{ij'k'}) = Cov_{A,B}=28.239 \approx  \rho_{A,B} \sigma_{A} \sigma_{B}
= .803 \times \sqrt{16.347} \times \sqrt{74.093} = 27.946$.  

\subsection{Comparing fm1 and fm3}

The sum of fm1's (Worker/Machine) ranefs ($b_{ij}+b_i$) are roughly the same as fm3's (Machine-1$\mid$ Worker) random effects $b_{ij}$ for each machine. \textbf{In other words, the random effect $b_i$ is folded into $b_{ij}$ in fm3.}

<<>>=
#fm1's ranefs summed up are 
## roughly the same as the fm3 ranefs:
matrix(unlist(ranef(fm1)$`Machine:Worker`),6,3) +
matrix(unlist(ranef(fm1)$Worker),6,3)
ranef(fm3)
@

\section{How the random effects are 'predicted' when using the ranef() command (section 4.4.3).}

In linear mixed models, we fit models like these (the Ware-Laird formulation--see Pinheiro and Bates 2000, for example):

\begin{equation} 
Y = X\beta + Zu + \epsilon
\end{equation}

Let $u\sim N(0,\sigma_u^2)$, and this is independent from $\epsilon\sim N(0,\sigma^2)$.  

Given $Y$, the ``minimum mean square error predictor'' of $u$ is the conditional expectation:

\begin{equation}
\hat{u} = E(u\mid Y)
\end{equation}

We can find $E(u\mid Y)$ as follows. We write the joint distribution of $Y$ and $u$ as:

\begin{equation}
\begin{pmatrix}
Y \\
u
\end{pmatrix}
= 
N\left(
\begin{pmatrix}
X\beta\\
0
\end{pmatrix},
\begin{pmatrix}
V_Y & C_{Y,u}\\
C_{u,Y} & V_u \\
\end{pmatrix}
\right)
\end{equation}

$V_Y, C_{Y,u}, C_{u,Y}, V_u$ are the various variance-covariance matrices. 
It is a fact (need to track this down) that

\begin{equation}
u\mid Y \sim N(C_{u,Y}V_Y^{-1}(Y-X\beta)), 
Y_u - C_{u,Y} V_Y^{-1} C_{Y,u})
\end{equation}

This apparently allows you to derive the BLUPs:

\begin{equation}
\hat{u}= C_{u,Y}V_Y^{-1}(Y-X\beta))
\end{equation}

Substituting $\hat{\beta}$ for $\beta$, we get:

\begin{equation}
BLUP(u)= \hat{u}(\hat{\beta})=C_{u,Y}V_Y^{-1}(Y-X\hat{\beta}))
\end{equation}

Here's an example with R:

<<>>=
# Calculate the predicted random effects by hand for the ergoStool data
(fm1<-lmer(effort~Type-1 + (1|Subject),ergoStool))

## Here are the BLUPs we will estimate by hand:
ranef(fm1)

## this gives us all the variance components:
VarCorr(fm1)

# First, calculate the predicted random effect for subject 1:

## The variance for the random effect subject is the term C_{u,Y}:
covar.u.y<-VarCorr(fm1)$Subject[1]


# Estimated covariance between u_1 and Y_1
## make up a var-covar matrix from this:
(cov.u.Y<-matrix(covar.u.y,1,4))


# Estimated variance matrix for Y_1
(V.Y<-matrix(1.7755,4,4)+diag(1.2106,4,4))

# Extract observations for subject 1
(Y<-matrix(ergoStool$effort[1:4],4,1))

# Estimated fixed effects
(beta.hat<-matrix(fixef(fm1),4,1))

# Predicted random effect
cov.u.Y %*% solve(V.Y)%*%(Y-beta.hat)

# Compare with ranef command
ranef(fm1)$Subject[1,1]

# Calculate predicted random effects for all subjects
t(cov.u.Y %*% solve(V.Y)%*%(matrix(ergoStool$effort,4,9)-matrix(fixef(fm1),4,9)))
ranef(fm1)
@

\section{Correlations of fixed effects}

For an ordinary linear model, the covariance matrix (from which we can get the correlation matrix) of $\hat{beta}$ is

\begin{equation}
\sigma^2 \times (X^T X)^{-1}.
\end{equation}

For a mixed effects model, the standard deviations (standard errors) and correlations for the fixed effects estimators are listed at the end of the lmer output. 

<<>>=
lm.full<-lmer(wear~material-1+(1|Subject), data = BHHshoes)
@

The estimated correlation between $\hat{beta}_1$ and $\hat{beta}_2$ is $0.988$.
In this case, we have simple forms for the parameter estimators:

\begin{equation}
\hat{\beta}_1 = (Y_{1,1} + Y_{2,1} + \dots + Y_{10,1})/10
\end{equation}


\begin{equation}
\hat{\beta}_2 = (Y_{1,2} + Y_{2,2} + \dots + Y_{10,2})/10
\end{equation}

<<>>=
b1.vals<-subset(BHHshoes,material=="A")$wear
b2.vals<-subset(BHHshoes,material=="B")$wear

vcovmatrix<-var(cbind(b1.vals,b2.vals))

covar<-vcovmatrix[1,2]
sds<-sqrt(diag(vcovmatrix))
covar/(sds[1]*sds[2])

#cf:
covar/((0.786*sqrt(10))^2)  
@

In a regular linear model version, we would have had:

<<>>=
fm.lm<-lm(wear~material-1,BHHshoes)
X<-model.matrix(fm.lm)
2.49^2*solve(t(X)%*%X)
@

because $Var(\hat{\beta}) = \hat{\sigma}^2 (X^T X)^{-1}$.

From this, see if you can work out the covariance, and where the estimated correlation comes from, using the remainder of the lmer output above.

<<>>=
b1.diffs<-b1.vals-mean(b1.vals)
b2.diffs<-b2.vals-mean(b2.vals)

b1.diffs<-b1.vals-mean(BHHshoes$wear)
b2.diffs<-b2.vals-mean(BHHshoes$wear)

covar<-t(b1.diffs)%*%b2.diffs

b1.sd<-sd(b1.vals)
b2.sd<-sd(b2.vals)

corr<-covar/(b1.sd*b2.sd)
@

How does this work for multiple factors?

<<>>=
m1<-lmer(effort~Type-1+(1|Subject),ergoStool)

T1.vals<-subset(ergoStool,Type=="T1")$effort
T2.vals<-subset(ergoStool,Type=="T2")$effort
T3.vals<-subset(ergoStool,Type=="T3")$effort
T4.vals<-subset(ergoStool,Type=="T4")$effort

vals<-cbind(T1.vals,T2.vals,T3.vals,T4.vals)

## compute variance covariance matrix:
vcovmat<-var(vals)
## get sd's of each level:
sds<-sqrt(diag(vcovmat))

## T1.T2 correlation, the sds come from the model fit:
1.7222/(1.728*1.728)
@

Note: Not sure if the above is correct (the case of multiple levels in a factor).

\section{$\sigma_b^2$ describes both between-block variance, and within block covariance}

Consider the following model:

\begin{equation}
Y_{ij} = b_i + e_{ij},
\end{equation}


with $b_i\sim N(0,\sigma^2_b)$, $e_{ij}~N(0,\sigma^2)$.

Now try this in R (corresponding to $\sigma=1, \sigma_b=100, i=1,2,3$ and $j=1,2,3$):

<<>>=
block<-gl(3,3)
## very small within group:
eij<-rnorm(9,0,1)
## very high between group variance:
ei<-rnorm(3,0,100)
y<-rep(ei,each=3)+eij
plot(block,y)

fm1<-lm(y~1)

aggregated<-tapply(y,block,mean)
agg.data<-data.frame(means=aggregated,block=factor(1:3))

fm1a<-lm(y~1,agg.data)

fm3<-lmer(y~1+(1|block))

a<-y[c(1,4,7)]
b<-y[c(1,4,7)+1]
c<-y[c(1,4,7)+2]
(cov(a,b)+cov(a,c)+cov(b,c))/3

## more like what we experience:
block<-gl(3,3)
## large within group:
eij<-rnorm(9,0,100)
## small between group:
ei<-rnorm(3,0,1)
y<-rep(ei,each=3)+eij
plot(block,y)

fm1<-lm(y~1)

aggregated<-tapply(y,block,mean)
agg.data<-data.frame(means=aggregated,block=factor(1:3))

fm1a<-lm(y~1,agg.data)

fm3<-lmer(y~1+(1|block))
@

Perhaps it's just worth remembering that a variance is a covariance of a random variable with itself, and then consider the model formulation. If we have

\begin{equation}
Y_{ij} = \mu + b_i + \epsilon_{ij}
\end{equation}

where i is the group, j is the replication, if we \textit{define} $b_i\sim N(0, \sigma^2_b)$, and refer to $\sigma^2_b$ as the between group variance, then we must have


\begin{equation}
\begin{split}
Cov(Y_{i1}, Y_{i2}) =& Cov(\mu + b_i + \epsilon_{i1} , \mu + b_i + \epsilon_{i2})\\
=& \explain{Cov(\mu, \mu)}{=0} + 
 \explain{Cov(\mu, b_i)}{=0} +
 \explain{Cov(\mu, \epsilon_{i2})}{=0} +
 \explain{Cov(b_i,\mu)}{=0} +
  \explain{Cov(b_i,b_i)}{+ve} \dots\\
  =&  Cov(b_i, b_i) = Var(b_i) = \sigma^2_b\\
\end{split}
\end{equation}


\bibliographystyle{plain}
\bibliography{/Users/shravanvasishth/Dropbox/Bibliography/bibcleaned}
